{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Activation,Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import model_from_yaml\n",
    "from random import randint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    def __init__(self, corpus_path,time_steps):\n",
    "        \n",
    "        self.time_steps_Tx  = time_steps\n",
    "\n",
    "        with open(corpus_path) as corpus_file:\n",
    "            self.corpus = corpus_file.read()\n",
    "        \n",
    "        self.corpus_len = len(self.corpus)\n",
    "        self.no_examples_m  = self.corpus_len - self.time_steps_Tx\n",
    "\n",
    "        self.vocab        = sorted(list(set(self.corpus)))\n",
    "        self.vocab_len_Vx = len(self.vocab)\n",
    "        \n",
    "        print(\"corpus sample = '{}' corpus len = {} vocab_len_Vx = {}\".\n",
    "              format(self.corpus[0:20],self.corpus_len,self.vocab_len_Vx))\n",
    "        \n",
    "        print(f'vocab values = {self.vocab}')\n",
    "\n",
    "        # Get a unique identifier for each char in the corpus,\n",
    "        # then make some dicts to ease encoding and decoding\n",
    "        \n",
    "        self.encoder = {c: i for i, c in enumerate(self.vocab)}\n",
    "        self.decoder = {i: c for i, c in enumerate(self.vocab)}\n",
    "\n",
    "    def get_dataset(self):\n",
    "        # First each char after Tx, we will have one example.\n",
    "        \n",
    "        feature = np.zeros((self.no_examples_m,self.time_steps_Tx))\n",
    "        label   = np.zeros(self.no_examples_m)\n",
    "        \n",
    "        for i in range (0, self.no_examples_m, 1):\n",
    "            sentence  = self.corpus[i:i + self.time_steps_Tx]\n",
    "            next_char = self.corpus[i + self.time_steps_Tx]\n",
    "            \n",
    "            for j in range(self.time_steps_Tx):\n",
    "                feature[i,j] = self.encoder[sentence[j]]\n",
    "            label[i] = self.encoder[next_char]\n",
    "\n",
    "        feature = to_categorical(feature)\n",
    "        label   = to_categorical(label)\n",
    "        print(\"Sliced our corpus into {} examples. feature.shape = {} label.shape = {}\".\n",
    "              format(self.no_examples_m, feature.shape,label.shape))\n",
    "        return (feature,label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transform X data (m,Tx,vec_size) to Y (m,1,vec_size) i.e (m,vec_size).\n",
    "- Many to One RNN architecture\n",
    "- Lets convert into one hot encoding\n",
    "- LSTM model will remove Tx dimension if you don't specify return_sequences=True.\n",
    "- In the predict, you can pass a random text of length upto Tx to kick start the prediction. Loop it after it gives each word.\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRNN:\n",
    "    \n",
    "    def __init__(self,time_steps_Tx,vocab_len_Vx,encoder,decoder):\n",
    "        self.time_steps_Tx = time_steps_Tx\n",
    "        self.vocab_len_Vx  = vocab_len_Vx\n",
    "        self.encoder       = encoder\n",
    "        self.decoder       = decoder\n",
    "        \n",
    "    def build(self,units,layers=1,dropout=None):\n",
    "        model = Sequential()\n",
    "\n",
    "        for i in range(layers):\n",
    "            if(layers == 1):\n",
    "                model.add(LSTM(units, input_shape=(self.time_steps_Tx, self.vocab_len_Vx)))\n",
    "            elif(i == 0): \n",
    "                model.add(LSTM(units, input_shape=(self.time_steps_Tx, self.vocab_len_Vx),return_sequences=True))\n",
    "            elif(i != layers -1):\n",
    "                model.add(LSTM(units, return_sequences=True))\n",
    "            else:\n",
    "                model.add(LSTM(units))\n",
    "\n",
    "            if(dropout is not None):\n",
    "                model.add(Dropout(dropout))\n",
    "                    \n",
    "        model.add(Dense(self.vocab_len_Vx))\n",
    "        model.add(Activation('softmax'))\n",
    "        \n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "        self.model = model\n",
    "        self.model.summary()\n",
    "    \n",
    "    def load(self,mfile,cpfile):\n",
    "        with open(mfile) as model_file:\n",
    "            architecture = model_file.read()\n",
    "\n",
    "        self.model = model_from_yaml(architecture)\n",
    "        self.model.load_weights(cpfile)\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "        self.model.summary()\n",
    "        \n",
    "    def train(self,mfile,cpfile,X_train,Y_train,epochs,batch_size):\n",
    "        architecture = self.model.to_yaml()\n",
    "        with open(mfile, 'w') as model_file:\n",
    "            model_file.write(architecture)\n",
    "\n",
    "        file_path= cpfile + \"-checkpoint-{epoch:02d}-{loss:.3f}.hdf5\"\n",
    "        checkpoint = ModelCheckpoint(file_path, monitor=\"loss\", verbose=1, save_best_only=True, mode=\"min\")\n",
    "        callbacks = [checkpoint]\n",
    "\n",
    "        self.model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, callbacks=callbacks)\n",
    "    \n",
    "    def pad_seed(self,seed_phrase):\n",
    "        phrase_length = len(seed_phrase)\n",
    "        pattern = \"\"\n",
    "        for i in range (0, self.time_steps_Tx):\n",
    "            pattern += seed_phrase[i % phrase_length]\n",
    "        return pattern\n",
    "\n",
    "    def generate(self, seed_text,text_length):\n",
    "        \n",
    "        X = np.zeros((1, self.time_steps_Tx, self.vocab_len_Vx), dtype=np.bool)\n",
    "        for i, character in enumerate(self.pad_seed(seed_text)):\n",
    "            X[0, i, self.encoder[character]] = 1\n",
    "\n",
    "        generated_text = \"\"\n",
    "        for i in range(text_length):\n",
    "            prediction = np.argmax(self.model.predict(X, verbose=0))\n",
    "\n",
    "            generated_text += self.decoder[prediction]\n",
    "\n",
    "            activations = np.zeros((1, 1, self.vocab_len_Vx), dtype=np.bool)\n",
    "            activations[0, 0, prediction] = 1\n",
    "            X = np.concatenate((X[:, 1:, :], activations), axis=1)\n",
    "\n",
    "        return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus sample = 'From fairest creatur' corpus len = 94651 vocab_len_Vx = 61\n",
      "vocab values = ['\\n', ' ', '!', \"'\", '(', ')', ',', '-', '.', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Sliced our corpus into 94601 examples. feature.shape = (94601, 50, 61) label.shape = (94601, 61)\n",
      "XE.shape =  (94601, 50, 61)\n",
      "YE.shape =  (94601, 61)\n"
     ]
    }
   ],
   "source": [
    "Tx = 50\n",
    "\n",
    "crp = Corpus('data/sonnets.txt',Tx)\n",
    "m  = crp.no_examples_m\n",
    "Vx = crp.vocab_len_Vx\n",
    "\n",
    "XE,YE = crp.get_dataset()\n",
    "print('XE.shape = ',XE.shape)\n",
    "print('YE.shape = ',YE.shape)\n",
    "\n",
    "#net = CRNN(Tx,Vx,crp.encoder,crp.decoder)\n",
    "#net.load('shekespere_model.yaml','shekespere_model_weights_final.hdf5')\n",
    "\n",
    "#seed = 'mallesh'\n",
    "\n",
    "#print(net.generate(seed,500))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "madhu \n",
      "gomathy \n",
      "sanjay\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
