{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Activation\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    def __init__(self, corpus_path,Tx):\n",
    "        with open(corpus_path) as corpus_file:\n",
    "            self.corpus = corpus_file.read()\n",
    "\n",
    "        # Get a unique identifier for each char in the corpus,\n",
    "        # then make some dicts to ease encoding and decoding\n",
    "        self.vocab = sorted(list(set(self.corpus)))\n",
    "        self.encoder = {c: i for i, c in enumerate(self.chars)}\n",
    "        self.decoder = {i: c for i, c in enumerate(self.chars)}\n",
    "\n",
    "        # Some fields we'll need later\n",
    "        self.vocab_len = len(self.chars)\n",
    "        self.Tx = Tx\n",
    "        self.corpus_len = len(self.corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus sample = 'From fairest creatur' corpus len = 94651\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/sonnets.txt\") as corpus_file:\n",
    "    corpus = corpus_file.read()\n",
    "print(\"corpus sample = '{}' corpus len = {}\".format(corpus[0:20],len(corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab = ['\\n', ' ', '!', \"'\", '(', ')', ',', '-', '.', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'] vocab_len = 61\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(list(set(corpus)))\n",
    "vocab_len = len(vocab)\n",
    "char_to_index = {c: i for i, c in enumerate(vocab)}\n",
    "index_to_char = {i: c for i, c in enumerate(vocab)}\n",
    "print(\"vocab = {} vocab_len = {}\".format(vocab,vocab_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a Dataset like below, for a copus <i><b> from fairest creatures we desire increase </b></i> with Tx = 10.\n",
    "-    X   ==>          Y\n",
    "- from faire ==>       s\n",
    "- rom faires ==>       t\n",
    "- om fairest ==>      ' '\n",
    "\n",
    "We can create corpus_len - Tx examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sliced our corpus into 94601 sentences of length 50\n"
     ]
    }
   ],
   "source": [
    "sentence_length = 50  # Tx\n",
    "X_train_raw = []\n",
    "Y_train_raw = []\n",
    "for i in range (0, len(corpus) - sentence_length, 1):\n",
    "    sentence = corpus[i:i + sentence_length]\n",
    "    next_char = corpus[i + sentence_length]\n",
    "    X_train_raw.append([char_to_index[char] for char in sentence])\n",
    "    Y_train_raw.append(char_to_index[next_char])\n",
    "\n",
    "num_sentences = len(X_train_raw)\n",
    "print(\"Sliced our corpus into {0} sentences of length {1}\".format(num_sentences, sentence_length))\n",
    "m = num_sentences\n",
    "Tx = sentence_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transform X data (m,Tx,vec_size) to Y (m,1,vec_size) i.e (m,vec_size).\n",
    "- Many to One RNN architecture\n",
    "- Lets convert into one hot encoding\n",
    "- LSTM model will remove Tx dimension if you don't specify return_sequences=True.\n",
    "- In the predict, you can pass a random text of length upto Tx to kick start the prediction. Loop it after it gives each word.\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing X and y...\n",
      "X_train.shape =  (94601, 50, 61) \n",
      "Y_train.shape =  (94601, 61)\n"
     ]
    }
   ],
   "source": [
    "# Vectorize our data and labels. We want everything in one-hot\n",
    "X_train = to_categorical(X_train_raw)\n",
    "Y_train = to_categorical(Y_train_raw)\n",
    "\n",
    "print('X_train.shape = ',X_train.shape,'\\nY_train.shape = ',Y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's build a brain!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/madhusudanan/anaconda3/envs/tf_py3.6/lib/python3.6/site-packages/keras/models.py:981: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "94601/94601 [==============================] - 222s 2ms/step - loss: 2.6186\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.61865, saving model to weights-01-2.619.hdf5\n",
      "Epoch 2/30\n",
      "94601/94601 [==============================] - 226s 2ms/step - loss: 2.1145\n",
      "\n",
      "Epoch 00002: loss improved from 2.61865 to 2.11454, saving model to weights-02-2.115.hdf5\n",
      "Epoch 3/30\n",
      "94601/94601 [==============================] - 216s 2ms/step - loss: 1.9553\n",
      "\n",
      "Epoch 00003: loss improved from 2.11454 to 1.95527, saving model to weights-03-1.955.hdf5\n",
      "Epoch 4/30\n",
      "94601/94601 [==============================] - 223s 2ms/step - loss: 1.8503\n",
      "\n",
      "Epoch 00004: loss improved from 1.95527 to 1.85029, saving model to weights-04-1.850.hdf5\n",
      "Epoch 5/30\n",
      "94601/94601 [==============================] - 220s 2ms/step - loss: 1.7688\n",
      "\n",
      "Epoch 00005: loss improved from 1.85029 to 1.76882, saving model to weights-05-1.769.hdf5\n",
      "Epoch 6/30\n",
      "94601/94601 [==============================] - 219s 2ms/step - loss: 1.7027\n",
      "\n",
      "Epoch 00006: loss improved from 1.76882 to 1.70265, saving model to weights-06-1.703.hdf5\n",
      "Epoch 7/30\n",
      "94601/94601 [==============================] - 225s 2ms/step - loss: 1.6459\n",
      "\n",
      "Epoch 00007: loss improved from 1.70265 to 1.64594, saving model to weights-07-1.646.hdf5\n",
      "Epoch 8/30\n",
      "94601/94601 [==============================] - 223s 2ms/step - loss: 1.5949\n",
      "\n",
      "Epoch 00008: loss improved from 1.64594 to 1.59491, saving model to weights-08-1.595.hdf5\n",
      "Epoch 9/30\n",
      "94601/94601 [==============================] - 220s 2ms/step - loss: 1.5488\n",
      "\n",
      "Epoch 00009: loss improved from 1.59491 to 1.54878, saving model to weights-09-1.549.hdf5\n",
      "Epoch 10/30\n",
      "94601/94601 [==============================] - 221s 2ms/step - loss: 1.5038\n",
      "\n",
      "Epoch 00010: loss improved from 1.54878 to 1.50384, saving model to weights-10-1.504.hdf5\n",
      "Epoch 11/30\n",
      "94601/94601 [==============================] - 221s 2ms/step - loss: 1.4630\n",
      "\n",
      "Epoch 00011: loss improved from 1.50384 to 1.46301, saving model to weights-11-1.463.hdf5\n",
      "Epoch 12/30\n",
      "94601/94601 [==============================] - 227s 2ms/step - loss: 1.4204\n",
      "\n",
      "Epoch 00012: loss improved from 1.46301 to 1.42041, saving model to weights-12-1.420.hdf5\n",
      "Epoch 13/30\n",
      "94601/94601 [==============================] - 227s 2ms/step - loss: 1.3778\n",
      "\n",
      "Epoch 00013: loss improved from 1.42041 to 1.37777, saving model to weights-13-1.378.hdf5\n",
      "Epoch 14/30\n",
      "94601/94601 [==============================] - 227s 2ms/step - loss: 1.3365\n",
      "\n",
      "Epoch 00014: loss improved from 1.37777 to 1.33649, saving model to weights-14-1.336.hdf5\n",
      "Epoch 15/30\n",
      "94601/94601 [==============================] - 224s 2ms/step - loss: 1.2946\n",
      "\n",
      "Epoch 00015: loss improved from 1.33649 to 1.29461, saving model to weights-15-1.295.hdf5\n",
      "Epoch 16/30\n",
      "94601/94601 [==============================] - 212s 2ms/step - loss: 1.2540\n",
      "\n",
      "Epoch 00016: loss improved from 1.29461 to 1.25401, saving model to weights-16-1.254.hdf5\n",
      "Epoch 17/30\n",
      "94601/94601 [==============================] - 225s 2ms/step - loss: 1.2112\n",
      "\n",
      "Epoch 00017: loss improved from 1.25401 to 1.21116, saving model to weights-17-1.211.hdf5\n",
      "Epoch 18/30\n",
      "94601/94601 [==============================] - 228s 2ms/step - loss: 1.1686\n",
      "\n",
      "Epoch 00018: loss improved from 1.21116 to 1.16860, saving model to weights-18-1.169.hdf5\n",
      "Epoch 19/30\n",
      "94601/94601 [==============================] - 225s 2ms/step - loss: 1.1257\n",
      "\n",
      "Epoch 00019: loss improved from 1.16860 to 1.12566, saving model to weights-19-1.126.hdf5\n",
      "Epoch 20/30\n",
      "94601/94601 [==============================] - 222s 2ms/step - loss: 1.0820\n",
      "\n",
      "Epoch 00020: loss improved from 1.12566 to 1.08198, saving model to weights-20-1.082.hdf5\n",
      "Epoch 21/30\n",
      "94601/94601 [==============================] - 227s 2ms/step - loss: 1.0416\n",
      "\n",
      "Epoch 00021: loss improved from 1.08198 to 1.04159, saving model to weights-21-1.042.hdf5\n",
      "Epoch 22/30\n",
      "94601/94601 [==============================] - 231s 2ms/step - loss: 0.9988\n",
      "\n",
      "Epoch 00022: loss improved from 1.04159 to 0.99883, saving model to weights-22-0.999.hdf5\n",
      "Epoch 23/30\n",
      "94601/94601 [==============================] - 230s 2ms/step - loss: 0.9587\n",
      "\n",
      "Epoch 00023: loss improved from 0.99883 to 0.95873, saving model to weights-23-0.959.hdf5\n",
      "Epoch 24/30\n",
      "94601/94601 [==============================] - 237s 3ms/step - loss: 0.9201\n",
      "\n",
      "Epoch 00024: loss improved from 0.95873 to 0.92015, saving model to weights-24-0.920.hdf5\n",
      "Epoch 25/30\n",
      "94601/94601 [==============================] - 237s 3ms/step - loss: 0.8844\n",
      "\n",
      "Epoch 00025: loss improved from 0.92015 to 0.88441, saving model to weights-25-0.884.hdf5\n",
      "Epoch 26/30\n",
      "94601/94601 [==============================] - 232s 2ms/step - loss: 0.8476\n",
      "\n",
      "Epoch 00026: loss improved from 0.88441 to 0.84761, saving model to weights-26-0.848.hdf5\n",
      "Epoch 27/30\n",
      "94601/94601 [==============================] - 233s 2ms/step - loss: 0.8122\n",
      "\n",
      "Epoch 00027: loss improved from 0.84761 to 0.81222, saving model to weights-27-0.812.hdf5\n",
      "Epoch 28/30\n",
      "94601/94601 [==============================] - 231s 2ms/step - loss: 0.7792\n",
      "\n",
      "Epoch 00028: loss improved from 0.81222 to 0.77917, saving model to weights-28-0.779.hdf5\n",
      "Epoch 29/30\n",
      "94601/94601 [==============================] - 215s 2ms/step - loss: 0.7474\n",
      "\n",
      "Epoch 00029: loss improved from 0.77917 to 0.74736, saving model to weights-29-0.747.hdf5\n",
      "Epoch 30/30\n",
      "94601/94601 [==============================] - 228s 2ms/step - loss: 0.7167\n",
      "\n",
      "Epoch 00030: loss improved from 0.74736 to 0.71670, saving model to weights-30-0.717.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12824e4e0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define our model\n",
    "print(\"Let's build a brain!\")\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(Tx, vocab_len)))\n",
    "model.add(Dense(vocab_len))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Dump our model architecture to a file so we can load it elsewhere\n",
    "architecture = model.to_yaml()\n",
    "with open('model.yaml', 'a') as model_file:\n",
    "    model_file.write(architecture)\n",
    "\n",
    "# Set up checkpoints\n",
    "file_path=\"weights-{epoch:02d}-{loss:.3f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor=\"loss\", verbose=1, save_best_only=True, mode=\"min\")\n",
    "callbacks = [checkpoint]\n",
    "\n",
    "# Action time! [Insert guitar solo here]\n",
    "model.fit(X_train, Y_train, epochs=30, batch_size=128, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
